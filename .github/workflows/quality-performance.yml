name: Quality & Performance Checks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  frontend-quality:
    name: Frontend Quality & Performance
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/employee/package-lock.json
          
      - name: Install dependencies
        working-directory: frontend/employee
        run: npm ci
        
      - name: TypeScript Type Check
        working-directory: frontend/employee
        run: npx tsc --noEmit
        
      - name: ESLint Check
        working-directory: frontend/employee
        run: npm run lint
        
      - name: Security Audit
        working-directory: frontend/employee
        run: |
          npm audit --audit-level=moderate
          # Allow moderate vulnerabilities but fail on high/critical
        continue-on-error: false
        
      - name: Build Performance Check
        working-directory: frontend/employee
        run: |
          echo "Starting build performance measurement..."
          start_time=$(date +%s)
          npm run build
          end_time=$(date +%s)
          build_time=$((end_time - start_time))
          echo "Build completed in ${build_time} seconds"
          
          # Fail if build takes longer than 120 seconds
          if [ $build_time -gt 120 ]; then
            echo "❌ Build time exceeded 120 seconds: ${build_time}s"
            exit 1
          else
            echo "✅ Build time within limits: ${build_time}s"
          fi
          
      - name: Bundle Size Analysis
        working-directory: frontend/employee
        run: |
          npm run build:analyze
          
          # Check if dist folder exists and get sizes
          if [ -d "dist" ]; then
            echo "📦 Bundle Analysis:"
            
            # Get main bundle size
            main_size=$(find dist/assets -name "index-*.js" -exec ls -lh {} \; | awk '{print $5}' | head -1)
            echo "Main bundle size: $main_size"
            
            # Get total dist size
            total_size=$(du -sh dist | awk '{print $1}')
            echo "Total bundle size: $total_size"
            
            # Get detailed breakdown
            echo "📊 Detailed breakdown:"
            find dist -type f -name "*.js" -o -name "*.css" | while read file; do
              size=$(ls -lh "$file" | awk '{print $5}')
              echo "  $(basename "$file"): $size"
            done
            
            # Check if any single JS file is larger than 1MB
            large_files=$(find dist -name "*.js" -size +1M)
            if [ ! -z "$large_files" ]; then
              echo "⚠️  Large files detected (>1MB):"
              echo "$large_files" | while read file; do
                size=$(ls -lh "$file" | awk '{print $5}')
                echo "  $file: $size"
              done
              echo "Consider code splitting or optimization"
            fi
          else
            echo "❌ No dist folder found after build"
            exit 1
          fi

  backend-quality:
    name: Backend Quality & Performance
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: backend/employee/requirements.txt
          
      - name: Install dependencies
        working-directory: backend/employee
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install django-debug-toolbar django-querycount pytest-django pytest-benchmark
          
      - name: Django Security Check
        working-directory: backend/employee
        env:
          DJANGO_SETTINGS_MODULE: core.settings_ci
          DATABASE_URL: postgres://testuser:testpass@localhost:5432/testdb
          REDIS_URL: redis://localhost:6379/0
        run: |
          python manage.py check --deploy --fail-level WARNING
          
      - name: Django System Check
        working-directory: backend/employee
        env:
          DJANGO_SETTINGS_MODULE: core.settings_ci
          DATABASE_URL: postgres://testuser:testpass@localhost:5432/testdb
          REDIS_URL: redis://localhost:6379/0
        run: |
          python manage.py check
          
      - name: Run Migrations
        working-directory: backend/employee
        env:
          DJANGO_SETTINGS_MODULE: core.settings_ci
          DATABASE_URL: postgres://testuser:testpass@localhost:5432/testdb
          REDIS_URL: redis://localhost:6379/0
        run: |
          python manage.py migrate
          
      - name: Performance & Query Analysis Tests
        working-directory: backend/employee
        env:
          DJANGO_SETTINGS_MODULE: core.settings_ci
          DATABASE_URL: postgres://testuser:testpass@localhost:5432/testdb
          REDIS_URL: redis://localhost:6379/0
        run: |
          echo "🔍 Running performance tests..."
          
          # Create test script for performance analysis
          cat > test_performance.py << 'EOF'
          import django
          import os
          import time
          from django.test import TestCase, override_settings
          from django.test.client import Client
          from django.contrib.auth.models import User
          from django.db import connection
          from django.conf import settings
          
          os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'core.settings_ci')
          django.setup()
          
          # Performance thresholds
          MAX_QUERIES_PER_VIEW = 10
          MAX_RESPONSE_TIME_MS = 500
          
          def analyze_view_performance(url, user=None, method='GET', data=None):
              client = Client()
              if user:
                  client.force_login(user)
              
              # Reset query count
              connection.queries_log.clear() if hasattr(connection, 'queries_log') else None
              
              start_time = time.time()
              
              if method == 'GET':
                  response = client.get(url)
              elif method == 'POST':
                  response = client.post(url, data or {})
              
              end_time = time.time()
              response_time_ms = (end_time - start_time) * 1000
              
              # Count queries
              query_count = len(connection.queries) if hasattr(connection, 'queries') else 0
              
              print(f"📊 {method} {url}")
              print(f"   Response time: {response_time_ms:.2f}ms")
              print(f"   Query count: {query_count}")
              print(f"   Status code: {response.status_code}")
              
              # Check thresholds
              issues = []
              if response_time_ms > MAX_RESPONSE_TIME_MS:
                  issues.append(f"Slow response: {response_time_ms:.2f}ms > {MAX_RESPONSE_TIME_MS}ms")
              
              if query_count > MAX_QUERIES_PER_VIEW:
                  issues.append(f"Too many queries: {query_count} > {MAX_QUERIES_PER_VIEW}")
              
              if issues:
                  print(f"   ⚠️  Issues: {', '.join(issues)}")
                  return False
              else:
                  print(f"   ✅ Performance OK")
                  return True
          
          def main():
              # Create test user
              user = User.objects.create_user('testuser', 'test@example.com', 'testpass')
              
              # Test endpoints
              endpoints_to_test = [
                  ('/api/health/', None, 'GET'),
                  ('/api/protected-endpoint/', user, 'GET'),
              ]
              
              all_passed = True
              print("🚀 Starting performance analysis...")
              
              for url, test_user, method in endpoints_to_test:
                  try:
                      passed = analyze_view_performance(url, test_user, method)
                      if not passed:
                          all_passed = False
                  except Exception as e:
                      print(f"   ❌ Error testing {url}: {str(e)}")
                      all_passed = False
              
              if all_passed:
                  print("\n✅ All performance tests passed!")
                  return 0
              else:
                  print("\n❌ Some performance tests failed!")
                  return 1
          
          if __name__ == '__main__':
              exit(main())
          EOF
          
          python test_performance.py
          
      - name: Security Linting with Bandit
        working-directory: backend/employee
        run: |
          pip install bandit[toml]
          
          # Run bandit security linting
          echo "🔒 Running security analysis..."
          bandit -r . -f json -o bandit-report.json || true
          
          # Display results
          if [ -f bandit-report.json ]; then
            echo "📋 Security scan results:"
            python -c "
          import json
          with open('bandit-report.json') as f:
              data = json.load(f)
              
          high_issues = [r for r in data.get('results', []) if r['issue_severity'] == 'HIGH']
          medium_issues = [r for r in data.get('results', []) if r['issue_severity'] == 'MEDIUM']
          
          print(f'High severity issues: {len(high_issues)}')
          print(f'Medium severity issues: {len(medium_issues)}')
          
          if high_issues:
              print('\n❌ High severity security issues found:')
              for issue in high_issues:
                  print(f'  - {issue[\"test_name\"]}: {issue[\"issue_text\"]}')
                  print(f'    File: {issue[\"filename\"]}:{issue[\"line_number\"]}')
              exit(1)
          elif medium_issues:
              print('\n⚠️  Medium severity security issues found:')
              for issue in medium_issues:
                  print(f'  - {issue[\"test_name\"]}: {issue[\"issue_text\"]}')
                  print(f'    File: {issue[\"filename\"]}:{issue[\"line_number\"]}')
              print('Consider reviewing these issues.')
          else:
              print('✅ No significant security issues found!')
            "
          fi

  integration-quality:
    name: Integration Quality Check
    runs-on: ubuntu-latest
    needs: [frontend-quality, backend-quality]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Docker Compose Validation
        run: |
          echo "🐳 Validating Docker configuration..."
          
          # Check if docker-compose files are valid
          docker-compose config
          
          # Check for common issues
          echo "📋 Docker configuration analysis:"
          
          # Check for exposed secrets
          if grep -r "password\|secret\|key" docker-compose.yml docker-compose.*.yml 2>/dev/null | grep -v "SECRET_KEY_FILE\|PASSWORD_FILE"; then
            echo "⚠️  Potential secrets in Docker files (review these):"
            grep -r "password\|secret\|key" docker-compose.yml docker-compose.*.yml 2>/dev/null | grep -v "SECRET_KEY_FILE\|PASSWORD_FILE"
          fi
          
          # Check for production-ready configurations
          if ! grep -q "restart:" docker-compose.yml 2>/dev/null; then
            echo "⚠️  Consider adding restart policies for production"
          fi
          
          echo "✅ Docker configuration validated"
          
      - name: API Documentation Check
        run: |
          echo "📚 Checking API documentation..."
          
          # Check if API documentation exists
          if [ -f "backend/employee/openapi.json" ] || [ -f "backend/employee/schema.yml" ]; then
            echo "✅ API documentation found"
          else
            echo "⚠️  No API documentation found. Consider adding OpenAPI/Swagger docs"
          fi
          
      - name: Environment Configuration Check
        run: |
          echo "🔧 Checking environment configuration..."
          
          # Check for .env.example
          if [ -f ".env.example" ]; then
            echo "✅ Environment example file found"
          else
            echo "⚠️  No .env.example file found. Consider adding one for setup instructions"
          fi
          
          # Check for sensitive files in git
          if [ -f ".env" ]; then
            echo "❌ .env file found in repository! This should be in .gitignore"
            exit 1
          fi
          
          echo "✅ Environment configuration check passed"

  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [frontend-quality, backend-quality, integration-quality]
    if: always()
    
    steps:
      - name: Performance Summary
        run: |
          echo "📊 Quality & Performance Check Summary"
          echo "=================================="
          
          # Check job statuses
          frontend_status="${{ needs.frontend-quality.result }}"
          backend_status="${{ needs.backend-quality.result }}"
          integration_status="${{ needs.integration-quality.result }}"
          
          echo "Frontend Quality: $frontend_status"
          echo "Backend Quality: $backend_status"
          echo "Integration Quality: $integration_status"
          
          # Overall status
          if [ "$frontend_status" = "success" ] && [ "$backend_status" = "success" ] && [ "$integration_status" = "success" ]; then
            echo ""
            echo "🎉 All quality & performance checks passed!"
            echo "✅ Code is ready for deployment"
          else
            echo ""
            echo "❌ Some quality checks failed"
            echo "🔧 Please review and fix the issues above"
            exit 1
          fi